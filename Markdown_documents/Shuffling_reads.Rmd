---
title: "Shuffling Simulated Reads"
author: "Jonathan Brenton"
date: "15/12/2019"
output:
  html_document:
    theme: readable
    highlight: tango
    toc: true
    toc_float: true
    css: C:/Users/JBrenton/Polyester_analysis_30_09_19/nibsc.report.css
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Shuffling Reads Generated from Simulations

For the simulations generated by either Flux Simulator or Polyester, the order of the reads need to be shuffled. Salmon requires reads to be in a random order. It is not clear if Polyester or Flux simulator are totally random in their generation of reads across the transcriptome or genome provided so we decided to shuffle the reads using a bash script. We adapted it from the shuffle script on the github page: https://github.com/mikelove/swimdown/blob/master/simulate/shuffle.sh

---

### Beginning of Slurm scheduler job script

```{bash options, include=TRUE, eval=FALSE}
#!/bin/bash
#SBATCH -N 1
#SBATCH -c 8
#SBATCH --mem=128g
#SBATCH --job-name shuffle-fasta

```

These are the settings for the job script (NB, the shuffling requires a high amount of memory and can crash if lower memory is requested). The above lines of code specify the bash shell to execute the script, the number of nodes, number of cores, total memory requested and the job name appearing in the scheduler, respectively.

---

## Evaluating filenames

```{bash filename, echo=TRUE, eval=FALSE}
fullfile01=$1
fullfile02=$2

fasta01=`basename $fullfile01`
fasta02=`basename $fullfile02`

shuffled01=${fasta01%.fasta.gz}_shuffled.fasta
shuffled02=${fasta02%.fasta.gz}_shuffled.fasta
```

The first two lines assign the input files to the names *fullfile01* and *fullfile02*. The next two lines mean that the files will have their prefixes removed with the *basename* bash function. The files then have their file extensions removed and the *_shuffled.fasta* extension added. So **/usr/share/sequencing/internships/JonathanBrenton/polyester_simulation/reads/sample_01_1.fasta.gz** becomes **sample_01_1_shuffled.fasta**.

---
 
## Changing Directories

```{bash go home, echo=TRUE, eval=FALSE}
cd /home/AD/jbrenton/fastashuffle/output/
mkdir -p ${fasta01%_1.fasta.gz}
cd ${fasta01%_1.fasta.gz}

```

The next code chunk changes the location of the working directory to a new folder, and makes a directory within that folder to perform the function/generate the shuffled reads.

## Unzipping the Fasta Files

```{bash unzip, echo=TRUE, eval=FALSE}
echo "#### unzipp the 2 fasta files"
date
pigz -p 8 -dc ${fullfile01} | sed 's/ /_/g' >${fasta01%.gz}
pigz -p 8 -dc ${fullfile02} | sed 's/ /_/g' >${fasta02%.gz}
echo "### unzip completed"
date
echo "######## files in the folder are"
ls -l *
```

The above code chunk unzips the the files using 8 cores (using the *-p 8* option, and the *-dc* options which cause pigz to decompress rather than compress the files and send the output of the files to standard output), removes the spaces and adds in an _ and sends the output into the *fasta* 01 and 02.

## Shuffling the reads

```{bash shuffling, echo=TRUE, eval=FALSE}
echo "#### shuffling the 2 fasta files"
date
cmd="paste ${fasta01%.gz} ${fasta02%.gz} | paste - - | shuf |   
awk '{print \$1 >\"$shuffled01\"; print \$3 >\"$shuffled01\"; 
print \$2 >\"$shuffled02\"; print \$4 >\"$shuffled02\"}'"
echo $cmd
eval $cmd
echo "#### shuffle completed"
date
echo "######## files in the folder are"
ls -l *
```

The main part of the code takes the two fasta files and pastes them twice, so the first and third columns will come from the first fasta file, and the second and fourth columns will come from the second/reverse fasta file. They both need to be pasted twice, as the first line contains the header and the second line contains the nucleotide sequence which need to be shuffled in the same order and later returned into the same file after shuffling. 

## Compressing Shuffled Files

```{bash compression, echo=TRUE, eval=FALSE}
echo "#### compressing files again"
date
pigz -p 8 ${shuffled01}
pigz -p 8 ${shuffled02}
echo "#### compressing completed"
date
echo "######## files in the folder are"
ls -l *
```

The shuffled reads are then compressed using the *pigz* command with the *-p* option enabled so the command uses 8 cores. 

## Cleaning Up

```{bash cleaning up, echo=TRUE, eval=FALSE}
echo "#### cleaning up"
rm ${fasta01%.gz} ${fasta02%.gz}
date
echo "######## files in the folder are"
ls -l *

echo "##### JOB COMPLETED"
date
```

The final section of the code deletes the intermediate files and confirms the job is finished.

---

# Command line entry to parallelize the script

```{bash command line entry, echo=TRUE, eval=FALSE}
for num in {0..40}
do
sbatch -o "./logs/shuffle_job_%j.out"  shuffle_reads_himem.job 
`pwd`/sample_${num}_1.fasta.gz `pwd`/sample_${num}_2.fasta.gz
done
```

This code can be directly input to the command line provided you are in the folder with the fasta files, to call the shuffle job. You need to have created a directory called *logs* for the output of the scheduler to send the log files.